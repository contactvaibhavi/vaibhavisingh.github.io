<!DOCTYPE HTML>
<html lang="en">
  <head>
    
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-D202Z93LJ3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-D202Z93LJ3');
    </script>

    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Vaibhavi Singh</title>
    <link rel="icon" type="image/png" href="assets/profile.png">
    <meta name="author" content="Vaibhavi Singh">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Graduate student in Computer Science at NYU Courant, specializing in spatial reasoning and planning in vision-language models. I study how VLMs solve complex visual and spatial problems through structured reasoning, with applications to embodied agents and robotic systems.">

<meta property="og:type" content="website">
<meta property="og:url" content="https://www.vaibhavisingh.com/">
<meta property="og:title" content="Vaibhavi Singh">
<meta property="og:description" content="Graduate student in Computer Science at NYU Courant, specializing in spatial reasoning and planning in vision-language models. I study how VLMs solve complex visual and spatial problems through structured reasoning, with applications to embodied agents and robotic systems.">
<meta property="og:image" content="https://www.vaibhavisingh.com/assets/profile.png">

<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:title" content="Vaibhavi Singh">
<meta name="twitter:description" content="Graduate student in Computer Science at NYU Courant, specializing in spatial reasoning and planning in vision-language models. I study how VLMs solve complex visual and spatial problems through structured reasoning, with applications to embodied agents and robotic systems.">
<meta name="twitter:image" content="https://www.vaibhavisingh.com/assets/profile.png">

    <style>
      body {
        font-family: 'Lato', Verdana, Helvetica, sans-serif;
        font-size: 14px;
        background-color: #fdfdfd;
        color: #333;
        margin: 0;
        padding: 0;
      }
      a {
        color: #1772d0;
        text-decoration: none;
      }
      a:focus, a:hover {
        color: #f09228;
        text-decoration: underline;
      }
      .name {
        font-size: 32px;
        font-weight: bold;
      }
      .papertitle {
        font-weight: 700;
        font-size: 14px;
      }
      h2 {
        font-size: 20px;
        font-weight: 400;
      }
      strong {
        font-weight: 700;
      }
      .highlight {
        background-color: #ffffd0;
      }
      .skill-category {
        margin-bottom: 8px;
      }
      
      .project-btn {
        display: inline-block;
        padding: 6px 16px;
        margin: 4px 4px 4px 0;
        border: 1px solid #ddd;
        border-radius: 4px;
        background-color: #fff;
        color: #333;
        text-decoration: none;
        font-size: 13px;
        transition: all 0.2s ease;
        }
      
      .project-btn:hover {
        background-color: #f5f5f5;
        border-color: #1772d0;
        color: #1772d0;
        text-decoration: none;
      }

      .project-container {
        display: flex;
        gap: 20px;
        align-items: flex-start;
        flex-direction: row;
        flex-wrap: nowrap; 
      }

      .project-image {
        flex-shrink: 0;
        width: 200px;
      }

      .project-image img {
        width: 100%;
        border-radius: 4px;
        border: 1px solid #ddd;
      }

      .project-content {
        flex: 1;
      }

      .project-keywords {
        color: #666;
        font-size: 13px;
        margin-top: 4px;
        margin-bottom: 8px;
      }

    </style>
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr style="padding:0px">
          <td style="padding:0px">
            
            <!-- Introduction Section -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr style="padding:0px">
                  <td style="padding:2.5%;width:63%;vertical-align:middle">
                    <p class="name" style="text-align: center;">
                      Vaibhavi Singh
                    </p>
                    <p>
                      I am a graduate student in Computer Science at <a href="https://cs.nyu.edu/">NYU Courant</a>, specializing in <strong>spatial reasoning</strong> and <strong>planning in vision-language models</strong>. I study how VLMs solve complex visual and spatial problems through structured reasoning, with applications to embodied agents and robotic systems.
                    </p>
                    
                    <p>
                      Prior to NYU, I built core spatial rendering and imaging systems at <a href="https://www.adobe.com" target="_blank">Adobe</a>, powering Photoshop, Substance 3D, and Creative Cloud — before moving through <a href="https://www.salesforce.com" target="_blank">Salesforce</a> and a healthcare AI startup, where I worked on learning from noisy temporal data in clinical applications.
                    </p>
                    <p style="text-align:center">
                      <a href="mailto:contactvaibhavisingh@gmail.com">Email</a> &nbsp;/&nbsp;
                      <a href="https://linkedin.com/in/contactvaibhavi">LinkedIn</a> &nbsp;/&nbsp;
                      <a href="https://github.com/contactvaibhavi">GitHub</a> &nbsp;/&nbsp;
                      <a href="https://twitter.com/__Vaibhavi">Twitter</a> &nbsp;/&nbsp;
                      <a href="writings.html">Writings</a>
                    </p>
                  </td>
                  <td style="padding:2.5%;width:37%;max-width:37%;vertical-align:middle;text-align:center;">
                    <div style="width:200px;height:200px;border-radius:50%;overflow:hidden;margin:0 auto;position:relative;">
                      <img src="assets/profile.png" 
                          alt="Vaibhavi Singh" 
                          style="height:100%;width:auto;position:absolute;left:50%;top:50%;transform:translate(-50%, -50%);">
                    </div>
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- Research Section -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:16px;width:100%;vertical-align:middle">
                    <h2>Research</h2>
                  </td>
                </tr>
              </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <!-- Research Project 1 -->
                <tr bgcolor="#ffffd0">
                  <td style="padding:16px;width:100%;vertical-align:middle">
                    <div class="project-container">
                     <div class="project-image">
                       <img src="assets/reasoning_project.png" alt="Reasoning Failures Project">
                     </div>

                     <div class="project-content">
                       <span class="papertitle">GVR-Bench: Probing Spatial Reasoning Limits in Vision-Language Models</span>
                       <div class="project-keywords">Geometric Reasoning, Visual Grounding, Spatial Transformations</div>
                      
                       <p>
                          Developed a systematic evaluation framework to probe the <strong>spatial reasoning capabilities</strong> of Vision-Language Models in deterministic settings. By engineering a suite of <strong>programmatic geometric tasks</strong> (e.g., precise rotations, spatial translations), demonstrated a critical dissociation between perceptual fidelity and logical execution. SOTA models achieved only <strong>16.8% pixel-level accuracy</strong> despite maintaining high perceptual similarity. Our analysis established a formal <strong>error taxonomy</strong>—classifying failures into <strong>geometric imprecision, grounding errors, and hallucinations</strong>—providing empirical evidence that current end-to-end architectures require <strong>hybrid spatial computation modules</strong> for precise robotic manipulation.
                       </p>
                        <a href="https://drive.google.com/file/d/1SaSQ542tDBfLy_nUmNmBHhZ6T-jmCP1A/view?usp=drive_link" class="project-btn">Paper</a>
                        <a href="https://github.com/contactvaibhavi/GVR-Bench" class="project-btn">Code</a>
                        <a href="https://docs.google.com/presentation/d/160EzrEZKZzGXa5641VY2UyGdO_MaN-6YpDFKl9feV0Y/edit?usp=sharing" class="project-btn">Slides</a>
                     </div>
                    </div>
                  </td>
                </tr>

                <!-- Research Project 2 -->
                <tr>
                  <td style="padding:16px;width:100%;vertical-align:middle">
                    <div class="project-container">
                    <div class="project-image">
                      <img src="assets/scaling_laws_project.png" alt="Scaling Laws Project">
                    </div>
                    
                    <div class="project-content">
                      <span class="papertitle">Tokenization Density vs. Scale: Dissecting Self-Supervised Learning Under Resource Constraints</span>
                      <div class="project-keywords">Vision Transformers, Spatial Tokenization, Self-Supervised Learning, Fine-Grained Recognition</div>
                      <p>
                        Trained self-supervised DiNOv1 models from scratch on low-resolution images (96×96 pixels) to investigate representation learning under severe resource constraints. Identified <strong>spatial tokenization density as the primary architectural bottleneck</strong>—switching from 16×16 patches (36 tokens/image) to 8×8 patches (144 tokens/image) improved fine-grained classification accuracy by 6 percentage points, outweighing gains from model depth or dataset scale. Demonstrated that <strong>strategically curated domain-aligned unlabeled data</strong> (74K task-relevant samples) provides superlinear returns compared to 10× larger generic datasets under compute constraints. Results reveal that feature tokenization granularity determines the discriminative capacity ceiling for constrained-resolution inputs, with implications for efficient vision encoders in robotics where spatial resolution and data efficiency are critical.
                      </p>
                     
                      <a href="https://docs.google.com/presentation/d/1AQx95qqQVj47TpJELMS9RmI_IXx-IZ6bmerbZid74nY/edit?usp=sharing" class="project-btn">Paper</a>
                      <!--a href="https://github.com/contactvaibhavi/Cogni-Reasoning" class="project-btn">Code</a-->
                      <a href="https://docs.google.com/presentation/d/1AQx95qqQVj47TpJELMS9RmI_IXx-IZ6bmerbZid74nY/edit?usp=sharing" class="project-btn">Slides</a>
                    </div>
                    </div>
                  </td>
                </tr>

              </tbody>
            </table>

            <!-- Academic Service Section -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:16px;width:100%;vertical-align:middle">
                    <h2>Academic Service</h2>
                  </td>
                </tr>
              </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:16px;width:100%;vertical-align:middle">
                    <b>NeurIPS 2025</b> &mdash; Ethics Reviewer (Datasets &amp; Benchmarks), Technical Reviewer (UniReps, ML4PS)
                    <br>
                    <b>ACL ARR 2026</b> &mdash; Reviewer
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- Experience Section -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:16px;width:100%;vertical-align:middle">
                    <h2>Experience</h2>
                  </td>
                </tr>
              </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>

                <tr>
                  <td style="padding:16px;width:100%;vertical-align:middle">
                    <strong>Machine Learning Engineer</strong>
                    <br>
                    Healthcare AI Startup, India
                    <br>
                    <em>2024 – 2025</em>
                    <p>
                      Built clinical risk prediction models (XGBoost, TCN) achieving 0.87 F1-score through feature engineering, SMOTE for class imbalance, & hyperparameter optimization. Processed sparse EMR data for early-stage healthcare applications.
                    </p>
                  </td>
                </tr>

                <tr>
                  <td style="padding:16px;width:100%;vertical-align:middle">
                    <strong>Software Engineer II</strong>
                    <br>
                    Salesforce, India
                    <br>
                    <em>2023 – 2024</em>
                    <p>
                      Engineered petabyte-scale data ingestion pipelines, reducing latency by 30% for Einstein AI & real-time analytics. Scaled multi-tenant Kubernetes infrastructure on AWS for 200+ microservices.
                    </p>
                  </td>
                </tr>

                <tr>
                  <td style="padding:16px;width:100%;vertical-align:middle">
                    <strong>ML Systems Engineer (MTS II)</strong>
                    <br>
                    Adobe, India
                    <br>
                    <em>2021 – 2023</em>
                    <p>
                      Optimized heterogeneous compute (CPU/GPU) architectures for Photoshop and Creative Cloud applications, reducing latency for 20M+ users. Extended core C++ image-processing engines to handle complex image analysis and color rendering, ensuring high-throughput performance under strict SLAs.
                    </p>
                  </td>
                </tr>

                <tr>
                  <td style="padding:16px;width:100%;vertical-align:middle">
                    <strong>Cloud Infrastructure Engineer (MTS I)</strong>
                    <br>
                    Adobe, India
                    <br>
                    <em>2019 – 2021</em>
                    <p>
                      Scaled distributed microservices for Adobe Cloud Platform, optimizing high-throughput request handling for 10M+ daily users. Reduced compute overhead by 12% through system-level performance profiling.
                    </p>
                  </td>
                </tr>

              </tbody>
            </table>

            <!-- Education Section -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:16px;width:100%;vertical-align:middle">
                    <h2>Education</h2>
                  </td>
                </tr>
              </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>

                <tr>
                  <td style="padding:16px;width:100%;vertical-align:middle">
                    <strong>M.S. Computer Science (Machine Learning)</strong>
                    <br>
                    New York University, Courant Institute
                    <br>
                    <em>2025 – 2027 (expected)</em>
                    <br>
                    GPA: 3.89/4.00
                    <p>
                      Research focus: spatial reasoning and planning in vision-language models
                    </p>
                    <p>
                      Coursework: Deep Learning (Yann LeCun), Computer Vision (Saining Xie), Natural Language Processing (Eunsol Choi)
                    </p>
                  </td>
                </tr>

                <tr>
                  <td style="padding:16px;width:100%;vertical-align:middle">
                    <strong>B.E. Computer Engineering (Hons)</strong>

                    <br>
                    Netaji Subhas Institute of Technology, University of Delhi
                    <br>
                    <em>2015 – 2019</em>
                    <br>
                    First Class with Distinction
                    <p>
                      Graduated in the top 10% of the department
                      <br>
                      Recipient of EPFL-Swiss Government scholarship (Scala Days 2019)
                      <br>
                      Google Summer of Code Mentor, Anita Borg Institute
                    </p>
                  </td>
                </tr>

              </tbody>
            </table>

            <!-- Footer -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px">
                    <br>
                    <p style="text-align:right;font-size:extra-small;">
                      Last updated: February 17, 2026
                      <br>
                      Website template from <a href="https://jonbarron.info/">Jon Barron</a>
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

          </td>
        </tr>
      </tbody>
    </table>
  </body>
</html>
